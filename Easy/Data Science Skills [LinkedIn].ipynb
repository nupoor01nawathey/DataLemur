{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c201d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|candidate_id|skills    |\n",
      "+------------+----------+\n",
      "|123         |Python    |\n",
      "|123         |Tableau   |\n",
      "|123         |PostgreSQL|\n",
      "|234         |R         |\n",
      "|234         |PowerBI   |\n",
      "|234         |SQL Server|\n",
      "|345         |Python    |\n",
      "|345         |Tableau   |\n",
      "+------------+----------+\n",
      "\n",
      "Using Dataframes -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [candidate_id#185 ASC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(candidate_id#185 ASC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=447]\n",
      "      +- Project [candidate_id#185]\n",
      "         +- Filter (total#201L = 3)\n",
      "            +- HashAggregate(keys=[candidate_id#185], functions=[count(candidate_id#185)])\n",
      "               +- Exchange hashpartitioning(candidate_id#185, 200), ENSURE_REQUIREMENTS, [plan_id=442]\n",
      "                  +- HashAggregate(keys=[candidate_id#185], functions=[partial_count(candidate_id#185)])\n",
      "                     +- Project [candidate_id#185]\n",
      "                        +- Filter skills#186 IN (Python,Tableau,PostgreSQL)\n",
      "                           +- Scan ExistingRDD[candidate_id#185,skills#186]\n",
      "\n",
      "\n",
      "+------------+\n",
      "|candidate_id|\n",
      "+------------+\n",
      "|123         |\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{IntegerType, StringType, StructType, StructField}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.Row\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(candidate_id,IntegerType,true),StructField(skills,StringType,true))\n",
       "data: Seq[org.apache.spark.sql.Row] = List([123,Python], [123,Tableau], [123,PostgreSQL], [234,R], [234,PowerBI], [234,SQL Server], [345,Python], [345,Tableau])\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[53] at parallelize at <console>:98\n",
       "df: org.apache.spark.sql.DataFrame = [candidate_id: int, skills: string]\n",
       "df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [candidate_id: int]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Given a table of candidates and their skills, you're tasked with finding the candidates best suited for an open Data Science job. \n",
    "// You want to find candidates who are proficient in Python, Tableau, and PostgreSQL.\n",
    "// Write a query to list the candidates who possess all of the required skills for the job. Sort the output by candidate ID in ascending order.\n",
    "// Assumption: There are no duplicates in the candidates table.\n",
    "// Example Output:\n",
    "// candidate_id\n",
    "// 123\n",
    "\n",
    "// Explanation\n",
    "// Candidate 123 is displayed because they have Python, Tableau, and PostgreSQL skills. \n",
    "// 345 isn't included in the output because they're missing one of the required skills: PostgreSQL.\n",
    "\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType, StructField}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "\n",
    "val schema = StructType(Array(\n",
    "                StructField(\"candidate_id\", IntegerType),\n",
    "                StructField(\"skills\", StringType)\n",
    "            ))\n",
    "\n",
    "val data = Seq(\n",
    "  Row(123,\"Python\"),\n",
    "  Row(123,\"Tableau\"),\n",
    "  Row(123,\"PostgreSQL\"),\n",
    "  Row(234,\"R\"),\n",
    "  Row(234,\"PowerBI\"),\n",
    "  Row(234,\"SQL Server\"),\n",
    "  Row(345,\"Python\"),\n",
    "  Row(345,\"Tableau\")\n",
    ")\n",
    "\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "df.show(false)\n",
    "\n",
    "println(\"Using Dataframes -------- \")\n",
    "val df1 = df.where($\"skills\".isin(\"Python\",\"Tableau\",\"PostgreSQL\")\n",
    "        ).groupBy($\"candidate_id\").agg(count($\"candidate_id\").as(\"total\")\n",
    "                                      ).select(\"candidate_id\").where($\"total\"===3\n",
    "                                                                    ).orderBy($\"candidate_id\".asc_nulls_last)\n",
    "df1.explain()\n",
    "df1.show(false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a6345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark SQL -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [candidate_id#185 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(candidate_id#185 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=643]\n",
      "      +- Project [candidate_id#185]\n",
      "         +- Filter (count(skills#186)#225L = 3)\n",
      "            +- HashAggregate(keys=[candidate_id#185], functions=[count(skills#186)])\n",
      "               +- Exchange hashpartitioning(candidate_id#185, 200), ENSURE_REQUIREMENTS, [plan_id=638]\n",
      "                  +- HashAggregate(keys=[candidate_id#185], functions=[partial_count(skills#186)])\n",
      "                     +- Filter skills#186 IN (Python,Tableau,PostgreSQL)\n",
      "                        +- Scan ExistingRDD[candidate_id#185,skills#186]\n",
      "\n",
      "\n",
      "+------------+\n",
      "|candidate_id|\n",
      "+------------+\n",
      "|123         |\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [candidate_id: int]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Using Spark SQL -------- \")\n",
    "\n",
    "df.createOrReplaceTempView(\"candidates\")\n",
    "\n",
    "val df2 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        candidate_id\n",
    "    FROM candidates\n",
    "    WHERE skills IN(\"Python\",\"Tableau\",\"PostgreSQL\")\n",
    "    GROUP BY candidate_id\n",
    "    HAVING COUNT(skills)=3\n",
    "    ORDER BY candidate_id\n",
    "\"\"\")\n",
    "\n",
    "df2.explain()\n",
    "df2.show(false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79596478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
