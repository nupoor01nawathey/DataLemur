{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8891c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+\n",
      "|user_id|device_type|view_time          |\n",
      "+-------+-----------+-------------------+\n",
      "|123    |tablet     |01/02/2022 00:00:00|\n",
      "|125    |laptop     |01/07/2022 00:00:00|\n",
      "|128    |laptop     |02/09/2022 00:00:00|\n",
      "|129    |phone      |02/09/2022 00:00:00|\n",
      "|145    |tablet     |02/24/2022 00:00:00|\n",
      "+-------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType}\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "data: Seq[org.apache.spark.sql.Row] = List([123,tablet,01/02/2022 00:00:00], [125,laptop,01/07/2022 00:00:00], [128,laptop,02/09/2022 00:00:00], [129,phone,02/09/2022 00:00:00], [145,tablet,02/24/2022 00:00:00])\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(user_id,IntegerType,true),StructField(device_type,StringType,true),StructField(view_time,StringType,true))\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[0] at parallelize at <console>:64\n",
       "df: org.apache.spark.sql.DataFrame = [user_id: int, device_type: string ... 1 more field]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Assume you're given the table on user viewership categorised by device type where the three types are laptop, tablet, and phone.\n",
    "\n",
    "// Write a query that calculates the total viewership for laptops and mobile devices where mobile is defined as the sum of tablet and phone viewership. \n",
    "// Output the total viewership for laptops as laptop_reviews and the total viewership for mobile devices as mobile_views.\n",
    "\n",
    "// Effective 15 April 2023, the solution has been updated with a more concise and easy-to-understand approach.\n",
    "\n",
    "// Example Output\n",
    "// ----------------------------\n",
    "// laptop_views | mobile_views\n",
    "// -------------|--------------\n",
    "// 2            | 3\n",
    "// ----------------------------\n",
    "\n",
    "// Explanation\n",
    "// Based on the example input, there are a total of 2 laptop views and 3 mobile views.\n",
    "\n",
    "// The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType}\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "\n",
    "val data = Seq(\n",
    "  Row(123,\"tablet\",\"01/02/2022 00:00:00\"),\n",
    "  Row(125,\"laptop\",\"01/07/2022 00:00:00\"),\n",
    "  Row(128,\"laptop\",\"02/09/2022 00:00:00\"),\n",
    "  Row(129,\"phone\",\"02/09/2022 00:00:00\"),\n",
    "  Row(145,\"tablet\",\"02/24/2022 00:00:00\")\n",
    ")\n",
    "\n",
    "val schema = StructType(Array(\n",
    "  StructField(\"user_id\", IntegerType),\n",
    "  StructField(\"device_type\", StringType),\n",
    "  StructField(\"view_time\", StringType)\n",
    "))\n",
    "\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "df.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c0d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Dataframes -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[device_type#347], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(device_type#347, 200), ENSURE_REQUIREMENTS, [plan_id=865]\n",
      "      +- HashAggregate(keys=[device_type#347], functions=[partial_count(1)])\n",
      "         +- Project [CASE WHEN (device_type#4 = tablet) THEN phone WHEN (device_type#4 = phone) THEN phone ELSE laptop END AS device_type#347]\n",
      "            +- Scan ExistingRDD[user_id#3,device_type#4,view_time#5]\n",
      "\n",
      "\n",
      "+------------+-----+\n",
      "|device_type |count|\n",
      "+------------+-----+\n",
      "|phone_views |3    |\n",
      "|laptop_views|2    |\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "df1: org.apache.spark.sql.DataFrame = [device_type: string, count: bigint]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "println(\"Using Dataframes -------- \")\n",
    "\n",
    "val df1 = df.withColumn(\"device_type\", when($\"device_type\" === lit(\"tablet\"), lit(\"phone\")\n",
    "                                  ).when($\"device_type\" === lit(\"phone\"), lit(\"phone\")).otherwise(\"laptop\")\n",
    "             ).groupBy($\"device_type\").count().withColumn(\"device_type\", concat($\"device_type\", lit(\"_views\")))\n",
    "\n",
    "\n",
    "df1.explain()\n",
    "df1.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ea2be29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark SQL  -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(CASE WHEN (device_type#289 = laptop) THEN 1 ELSE 0 END), sum(CASE WHEN ((device_type#289 = tablet) OR (device_type#289 = phone)) THEN 1 ELSE 0 END)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1329]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(CASE WHEN (device_type#289 = laptop) THEN 1 ELSE 0 END), partial_sum(CASE WHEN ((device_type#289 = tablet) OR (device_type#289 = phone)) THEN 1 ELSE 0 END)])\n",
      "         +- Project [device_type#289]\n",
      "            +- Scan ExistingRDD[user_id#288,device_type#289,view_time#290]\n",
      "\n",
      "\n",
      "+------------+------------+\n",
      "|laptop_views|mobile_views|\n",
      "+------------+------------+\n",
      "|2           |3           |\n",
      "+------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [laptop_views: bigint, mobile_views: bigint]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Using Spark SQL  -------- \")\n",
    "\n",
    "df.createOrReplaceTempView(\"viewership\")\n",
    "\n",
    "val df2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      SUM(CASE WHEN device_type='laptop' THEN 1 ELSE 0 END) AS laptop_views,\n",
    "      SUM(CASE WHEN device_type='tablet' OR device_type='phone' THEN 1 ELSE 0 END) AS mobile_views\n",
    "    FROM viewership\n",
    "\"\"\")\n",
    "\n",
    "df2.explain()\n",
    "df2.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52104e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
