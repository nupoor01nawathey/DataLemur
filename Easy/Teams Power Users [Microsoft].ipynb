{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d83a1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------------------+-------------------+\n",
      "|message_id|sender_id|receiver_id|content                |sent_date          |\n",
      "+----------+---------+-----------+-----------------------+-------------------+\n",
      "|901       |3601     |4500       |You up?                |2022-08-03 00:00:00|\n",
      "|902       |4500     |3601       |Only if you're buying  |2022-08-03 00:00:00|\n",
      "|743       |3601     |8752       |Let's take this offline|2022-06-14 00:00:00|\n",
      "|922       |3601     |4500       |Get on the call        |2022-08-10 00:00:00|\n",
      "+----------+---------+-----------+-----------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.Row\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(message_id,IntegerType,true),StructField(sender_id,IntegerType,true),StructField(receiver_id,IntegerType,true),StructField(content,StringType,true),StructField(sent_date,StringType,true))\n",
       "data: Seq[org.apache.spark.sql.Row] = List([901,3601,4500,You up?,2022-08-03 00:00:00], [902,4500,3601,Only if you're buying,2022-08-03 00:00:00], [743,3601,8752,Let's take this offline,2022-06-14 00:00:00], [922,3601,4500,Get on the call,2022-08-10 00:00:00])\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[24] at parallelize at <console>:82\n",
       "df: org.apache.spark.sql.DataFrame = [message_id: int,...\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Write a query to identify the top 2 Power Users who sent the highest number of messages on Microsoft Teams \n",
    "// in August 2022. Display the IDs of these 2 users along with the total number of messages they sent. \n",
    "// Output the results in descending order based on the count of the messages.\n",
    "\n",
    "// Assumption:\n",
    "// No two users have sent the same number of messages in August 2022.\n",
    "\n",
    "// Example Output:\n",
    "//------------------------------\n",
    "// sender_id | message_count\n",
    "//-----------|-------------------\n",
    "// 3601\t     |     2\n",
    "// 4500\t     |     1\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "\n",
    "val schema = StructType(Array(\n",
    "  StructField(\"message_id\", IntegerType),\n",
    "  StructField(\"sender_id\", IntegerType),\n",
    "  StructField(\"receiver_id\", IntegerType),\n",
    "  StructField(\"content\", StringType),\n",
    "  StructField(\"sent_date\", StringType)\n",
    "))\n",
    "\n",
    "val data = Seq(\n",
    "  Row(901,3601,4500,\"You up?\",\"2022-08-03 00:00:00\"),\n",
    "  Row(902,4500,3601,\"Only if you're buying\",\"2022-08-03 00:00:00\"),\n",
    "  Row(743,3601,8752,\"Let's take this offline\",\"2022-06-14 00:00:00\"),\n",
    "  Row(922,3601,4500,\"Get on the call\",\"2022-08-10 00:00:00\")\n",
    ")\n",
    "\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val df = spark.createDataFrame(rdd, schema).withColumn(\"sent_date\", to_timestamp($\"sent_date\"))\n",
    "\n",
    "\n",
    "df.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f6243dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Dataframes -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=2, orderBy=[message_count#311L DESC NULLS LAST], output=[sender_id#227,message_count#311L])\n",
      "   +- HashAggregate(keys=[sender_id#227], functions=[count(message_id#226)])\n",
      "      +- Exchange hashpartitioning(sender_id#227, 200), ENSURE_REQUIREMENTS, [plan_id=221]\n",
      "         +- HashAggregate(keys=[sender_id#227], functions=[partial_count(message_id#226)])\n",
      "            +- Project [message_id#226, sender_id#227]\n",
      "               +- Filter (isnotnull(sent_date#230) AND ((cast(sent_date#230 as timestamp) >= 2022-08-01 00:00:00) AND (cast(sent_date#230 as timestamp) <= 2022-08-31 00:00:00)))\n",
      "                  +- Scan ExistingRDD[message_id#226,sender_id#227,receiver_id#228,content#229,sent_date#230]\n",
      "\n",
      "\n",
      "+---------+-------------+\n",
      "|sender_id|message_count|\n",
      "+---------+-------------+\n",
      "|3601     |2            |\n",
      "|4500     |1            |\n",
      "+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sender_id: int, message_count: bigint]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Using Dataframes -------- \")\n",
    "\n",
    "val df1 = df.filter($\"sent_date\".between(\"2022-08-01 00:00:00\", \"2022-08-31 00:00:00\")\n",
    "                   ).groupBy($\"sender_id\").agg(count($\"message_id\").as(\"message_count\")\n",
    "                                              ).orderBy($\"message_count\".desc).limit(2)\n",
    "\n",
    "\n",
    "df1.explain()\n",
    "df1.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec4948d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark SQL -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=2, orderBy=[message_count#326L DESC NULLS LAST], output=[sender_id#227,message_count#326L])\n",
      "   +- HashAggregate(keys=[sender_id#227], functions=[count(message_id#226)])\n",
      "      +- Exchange hashpartitioning(sender_id#227, 200), ENSURE_REQUIREMENTS, [plan_id=301]\n",
      "         +- HashAggregate(keys=[sender_id#227], functions=[partial_count(message_id#226)])\n",
      "            +- Project [message_id#226, sender_id#227]\n",
      "               +- Filter (isnotnull(sent_date#230) AND ((cast(sent_date#230 as timestamp) >= 2022-08-01 00:00:00) AND (cast(sent_date#230 as timestamp) <= 2022-08-31 00:00:00)))\n",
      "                  +- Scan ExistingRDD[message_id#226,sender_id#227,receiver_id#228,content#229,sent_date#230]\n",
      "\n",
      "\n",
      "+---------+-------------+\n",
      "|sender_id|message_count|\n",
      "+---------+-------------+\n",
      "|3601     |2            |\n",
      "|4500     |1            |\n",
      "+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [sender_id: int, message_count: bigint]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Using Spark SQL -------- \")\n",
    "\n",
    "df.createOrReplaceTempView(\"messages\")\n",
    "\n",
    "val df2 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sender_id,\n",
    "        COUNT(message_id) message_count\n",
    "    FROM messages\n",
    "    WHERE sent_date BETWEEN '2022-08-01 00:00:00' AND '2022-08-31 00:00:00'\n",
    "    GROUP BY sender_id \n",
    "    ORDER BY message_count DESC\n",
    "    LIMIT 2\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df2.explain()\n",
    "df2.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf716314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
