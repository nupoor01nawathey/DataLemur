{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f3821ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|page_id|page_name             |\n",
      "+-------+----------------------+\n",
      "|20001  |SQL Solutions         |\n",
      "|20045  |Brain Exercises       |\n",
      "|20701  |Tips for Data Analysts|\n",
      "+-------+----------------------+\n",
      "\n",
      "+-------+-------+-------------------+\n",
      "|user_id|page_id|liked_date         |\n",
      "+-------+-------+-------------------+\n",
      "|111    |20001  |04/08/2022 00:00:00|\n",
      "|121    |20045  |03/12/2022 00:00:00|\n",
      "|156    |20001  |07/25/2022 00:00:00|\n",
      "+-------+-------+-------------------+\n",
      "\n",
      "Using Dataframes -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [page_id#648]\n",
      "   +- Filter isnull(page_id#656)\n",
      "      +- SortMergeJoin [page_id#648], [page_id#656], LeftOuter\n",
      "         :- Sort [page_id#648 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(page_id#648, 200), ENSURE_REQUIREMENTS, [plan_id=1057]\n",
      "         :     +- Project [page_id#648]\n",
      "         :        +- Scan ExistingRDD[page_id#648,page_name#649]\n",
      "         +- Sort [page_id#656 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(page_id#656, 200), ENSURE_REQUIREMENTS, [plan_id=1058]\n",
      "               +- Project [page_id#656]\n",
      "                  +- Filter isnotnull(page_id#656)\n",
      "                     +- Scan ExistingRDD[user_id#655,page_id#656,liked_date#657]\n",
      "\n",
      "\n",
      "+-------+\n",
      "|page_id|\n",
      "+-------+\n",
      "|20701  |\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType}\n",
       "pages_data: Seq[org.apache.spark.sql.Row] = List([20001,SQL Solutions], [20045,Brain Exercises], [20701,Tips for Data Analysts])\n",
       "pages_schema: org.apache.spark.sql.types.StructType = StructType(StructField(page_id,IntegerType,true),StructField(page_name,StringType,true))\n",
       "page_likes_data: Seq[org.apache.spark.sql.Row] = List([111,20001,04/08/2022 00:00:00], [121,20045,03/12/2022 00:00:00], [156,20001,07/25/2022 00:00:00])\n",
       "page_likes_schema: org.apache.spark.sql.types.StructType = StructType(StructField(user_id,IntegerType,true),StructField(page_id,IntegerType,true),StructField(liked_date,StringType,true))\n",
       "pages_rdd: org.apache.spark....\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Assume you're given two tables containing data about Facebook Pages and their respective likes (as in \"Like a Facebook Page\").\n",
    "\n",
    "// Write a query to return the IDs of the Facebook pages that have zero likes. The output should be sorted in ascending order based on the page IDs.\n",
    "\n",
    "// Example Output:\n",
    "// page_id\n",
    "// 20701\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType}\n",
    "\n",
    "\n",
    "val pages_data = Seq(\n",
    "  Row(20001,\"SQL Solutions\"),\n",
    "  Row(20045,\"Brain Exercises\"),\n",
    "  Row(20701,\"Tips for Data Analysts\")\n",
    ")\n",
    "\n",
    "val pages_schema = StructType(Array(\n",
    "    StructField(\"page_id\", IntegerType),\n",
    "    StructField(\"page_name\", StringType)\n",
    "  )\n",
    ")\n",
    "\n",
    "val page_likes_data = Seq(\n",
    "  Row(111,20001,\"04/08/2022 00:00:00\"),\n",
    "  Row(121,20045,\"03/12/2022 00:00:00\"),\n",
    "  Row(156,20001,\"07/25/2022 00:00:00\")\n",
    ")\n",
    "\n",
    "val page_likes_schema = StructType(Array(\n",
    "  StructField(\"user_id\", IntegerType),\n",
    "  StructField(\"page_id\", IntegerType),\n",
    "  StructField(\"liked_date\", StringType)\n",
    "))\n",
    "\n",
    "val pages_rdd = spark.sparkContext.parallelize(pages_data)\n",
    "val pages_df = spark.createDataFrame(pages_rdd, pages_schema)\n",
    "\n",
    "val likes_rdd = spark.sparkContext.parallelize(page_likes_data)\n",
    "val likes_df = spark.createDataFrame(likes_rdd, page_likes_schema)\n",
    "\n",
    "\n",
    "pages_df.show(false)\n",
    "likes_df.show(false)\n",
    "\n",
    "\n",
    "println(\"Using Dataframes -------- \")\n",
    "\n",
    "val df1 = pages_df.as(\"p\").join(likes_df.as(\"l\"), $\"l.page_id\" === $\"p.page_id\", \"left\"\n",
    "                               ).where($\"l.page_id\".isNull).select(\"p.page_id\")\n",
    "\n",
    "\n",
    "df1.explain()\n",
    "df1.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04eb9ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark SQL -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [page_id#648]\n",
      "   +- Filter isnull(page_id#656)\n",
      "      +- SortMergeJoin [page_id#648], [page_id#656], LeftOuter\n",
      "         :- Sort [page_id#648 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(page_id#648, 200), ENSURE_REQUIREMENTS, [plan_id=1695]\n",
      "         :     +- Project [page_id#648]\n",
      "         :        +- Scan ExistingRDD[page_id#648,page_name#649]\n",
      "         +- Sort [page_id#656 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(page_id#656, 200), ENSURE_REQUIREMENTS, [plan_id=1696]\n",
      "               +- Project [page_id#656]\n",
      "                  +- Filter isnotnull(page_id#656)\n",
      "                     +- Scan ExistingRDD[user_id#655,page_id#656,liked_date#657]\n",
      "\n",
      "\n",
      "+-------+\n",
      "|page_id|\n",
      "+-------+\n",
      "|20701  |\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [page_id: int]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Using Spark SQL -------- \")\n",
    "\n",
    "pages_df.createOrReplaceTempView(\"pages\")\n",
    "likes_df.createOrReplaceTempView(\"likes\")\n",
    "\n",
    "val df2 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        p.page_id\n",
    "    FROM pages p\n",
    "    LEFT JOIN likes l\n",
    "    ON p.page_id=l.page_id\n",
    "    WHERE l.page_id IS NULL\n",
    "\"\"\")\n",
    "\n",
    "df2.explain()\n",
    "\n",
    "df2.show(false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd82b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
