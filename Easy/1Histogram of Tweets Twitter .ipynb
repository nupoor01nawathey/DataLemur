{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8e6c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------------------------------------------------------------+-------------------+\n",
      "|tweet_id|user_id|msg                                                              |tweet_date         |\n",
      "+--------+-------+-----------------------------------------------------------------+-------------------+\n",
      "|214252  |111    |Am considering taking Tesla private at $420. Funding secured.    |12/30/2021 00:00:00|\n",
      "|739252  |111    |Despite the constant negative press covfefe                      |01/01/2022 00:00:00|\n",
      "|846402  |111    |Following @NickSinghTech on Twitter changed my life!             |02/14/2022 00:00:00|\n",
      "|241425  |254    |If the salary is so competitive why won’t you tell me what it is?|03/01/2022 00:00:00|\n",
      "|231574  |148    |I no longer have a manager. I can't be managed                   |03/23/2022 00:00:00|\n",
      "+--------+-------+-----------------------------------------------------------------+-------------------+\n",
      "\n",
      "Using Dataframes -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[tweets#848L], functions=[count(user_id#818)])\n",
      "   +- Exchange hashpartitioning(tweets#848L, 200), ENSURE_REQUIREMENTS, [plan_id=1027]\n",
      "      +- HashAggregate(keys=[tweets#848L], functions=[partial_count(user_id#818)])\n",
      "         +- HashAggregate(keys=[user_id#818], functions=[count(tweet_id#817)])\n",
      "            +- Exchange hashpartitioning(user_id#818, 200), ENSURE_REQUIREMENTS, [plan_id=1023]\n",
      "               +- HashAggregate(keys=[user_id#818], functions=[partial_count(tweet_id#817)])\n",
      "                  +- Project [tweet_id#817, user_id#818]\n",
      "                     +- Filter (isnotnull(tweet_date#820) AND ((tweet_date#820 >= 01/01/2022  00:00:00) AND (tweet_date#820 <= 12/31/2022 00:00:00)))\n",
      "                        +- Scan ExistingRDD[tweet_id#817,user_id#818,msg#819,tweet_date#820]\n",
      "\n",
      "\n",
      "+------+---------+\n",
      "|tweets|users_num|\n",
      "+------+---------+\n",
      "|1     |2        |\n",
      "|3     |1        |\n",
      "+------+---------+\n",
      "\n",
      "Using Spark SQL -------- \n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[tweet_count_per_user#877L], functions=[count(user_id#818)])\n",
      "   +- Exchange hashpartitioning(tweet_count_per_user#877L, 200), ENSURE_REQUIREMENTS, [plan_id=1164]\n",
      "      +- HashAggregate(keys=[tweet_count_per_user#877L], functions=[partial_count(user_id#818)])\n",
      "         +- HashAggregate(keys=[user_id#818], functions=[count(tweet_id#817)])\n",
      "            +- Exchange hashpartitioning(user_id#818, 200), ENSURE_REQUIREMENTS, [plan_id=1160]\n",
      "               +- HashAggregate(keys=[user_id#818], functions=[partial_count(tweet_id#817)])\n",
      "                  +- Project [tweet_id#817, user_id#818]\n",
      "                     +- Filter (isnotnull(tweet_date#820) AND ((tweet_date#820 >= 01/01/2022  00:00:00) AND (tweet_date#820 <= 12/31/2022 00:00:00)))\n",
      "                        +- Scan ExistingRDD[tweet_id#817,user_id#818,msg#819,tweet_date#820]\n",
      "\n",
      "\n",
      "+------------+---------+\n",
      "|tweet_bucket|users_num|\n",
      "+------------+---------+\n",
      "|1           |2        |\n",
      "|3           |1        |\n",
      "+------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{IntegerType, StringType, StructType, StructField}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.Row\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(tweet_id,IntegerType,true),StructField(user_id,IntegerType,true),StructField(msg,StringType,true),StructField(tweet_date,StringType,true))\n",
       "data: Seq[org.apache.spark.sql.Row] = List([214252,111,Am considering taking Tesla private at $420. Funding secured.,12/30/2021 00:00:00], [739252,111,Despite the constant negative press covfefe,01/01/2022 00:00:00], [846402,111,Following @NickSinghTech on Twitter changed my life!,02/14/2022 00:00:00], [241425,254,If the salary is so competitive why won’t you tell me what it is?,03/01/2022 00:00:00], [231574,148,I no longer ha...\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Assume you're given a table Twitter tweet data, write a query to obtain a histogram of tweets posted per user in 2022. Output the tweet count per user as the bucket and the number of Twitter users who fall into that bucket.\n",
    "\n",
    "// In other words, group the users by the number of tweets they posted in 2022 and count the number of users in each group.\n",
    "\n",
    "// Example Output:\n",
    "// |tweet_bucket | users_num |\n",
    "// ---------------------------\n",
    "// |1            | 2         |\n",
    "// |2            | 1         |\n",
    "\n",
    "// Explanation:\n",
    "// Based on the example output, there are two users who posted only one tweet in 2022, and one user who posted two tweets in 2022. The query groups the users by the number of tweets they posted and displays the number of users in each group.\n",
    "\n",
    "// The dataset you are querying against may have different input & output - this is just an example!\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType, StructField}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val schema = StructType(Array(\n",
    "  StructField(\"tweet_id\", IntegerType),\n",
    "  StructField(\"user_id\", IntegerType),\n",
    "  StructField(\"msg\", StringType),\n",
    "  StructField(\"tweet_date\", StringType))\n",
    ")\n",
    "\n",
    "val data = Seq(\n",
    "    Row(214252,111,\"Am considering taking Tesla private at $420. Funding secured.\",\"12/30/2021 00:00:00\"),\n",
    "    Row(739252,111,\"Despite the constant negative press covfefe\",\"01/01/2022 00:00:00\"),\n",
    "    Row(846402,111,\"Following @NickSinghTech on Twitter changed my life!\",\"02/14/2022 00:00:00\"),\n",
    "    Row(241425,254,\"If the salary is so competitive why won’t you tell me what it is?\",\"03/01/2022 00:00:00\"),\n",
    "    Row(231574,148,\"I no longer have a manager. I can't be managed\",\"03/23/2022 00:00:00\")\n",
    ")\n",
    "\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "df.show(false)\n",
    "\n",
    "println(\"Using Dataframes -------- \")\n",
    "val df1 = df.filter($\"tweet_date\".between(\"01/01/2022  00:00:00\", \"12/31/2022 00:00:00\")\n",
    "         ).groupBy($\"user_id\"\n",
    "                  ).agg(count(\"tweet_id\").as(\"tweets\")\n",
    "                       ).groupBy($\"tweets\").agg(count(\"user_id\").as(\"users_num\"))\n",
    "\n",
    "df1.explain()\n",
    "df1.show(false)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "println(\"Using Spark SQL -------- \")\n",
    "df.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "val df2 = spark.sql(\"\"\"SELECT \n",
    "  tweet_count_per_user AS tweet_bucket, \n",
    "  COUNT(user_id) AS users_num \n",
    "FROM (\n",
    "  SELECT \n",
    "    user_id, \n",
    "    COUNT(tweet_id) AS tweet_count_per_user \n",
    "  FROM tweets \n",
    "  WHERE tweet_date BETWEEN '01/01/2022  00:00:00' \n",
    "    AND '12/31/2022 00:00:00'\n",
    "  GROUP BY user_id) AS total_tweets \n",
    "GROUP BY tweet_count_per_user\"\"\")\n",
    "\n",
    "df2.explain()\n",
    "df2.show(false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68b790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
